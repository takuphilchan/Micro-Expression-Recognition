{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xVFvUUh-vTY"
   },
   "source": [
    "This is the training colab notebook, the original github page is https://github.com/takuphilchan/Micro-Expression-Recognition.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6teXKdi6Nbx8",
    "outputId": "09cb265b-782b-4cae-dd17-c31c9335cd06"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPD5QMSI2oyz",
    "outputId": "66ecd641-59a4-477a-a7fa-848ea02b6441"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sc6m849d0SxJ",
    "outputId": "938a6943-db57-4677-d370-f222e878d9e5"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Third-party library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch-related imports\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, WeightedRandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Sklearn-related imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, confusion_matrix, accuracy_score,\n",
    "                             balanced_accuracy_score, precision_recall_fscore_support, roc_auc_score, \n",
    "                             roc_curve, auc, precision_recall_curve)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# OpenCV and MediaPipe imports\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# torchvision-related imports\n",
    "from torchvision import transforms\n",
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights, r3d_18, R3D_18_Weights\n",
    "\n",
    "# Suppressing specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"multiprocessing.popen_fork\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"google.protobuf.symbol_database\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)  # Enable memory-efficient SDP\n",
    "torch.backends.cuda.enable_flash_sdp(True)          # Enable Flash SDP for best performance\n",
    "torch.backends.cuda.enable_math_sdp(False)          # Disable math SDP (assuming you don't need the extra stability)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "class FullDatasetCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cache.get(idx, (None, None))\n",
    "\n",
    "    def __setitem__(self, idx, item):\n",
    "        self.cache[idx] = item\n",
    "\n",
    "    def clear(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataset_folder, transform=None, seed=None, device=torch.device('cpu')):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.class_folders = sorted([folder for folder in os.listdir(self.dataset_folder) if os.path.isdir(os.path.join(self.dataset_folder, folder))])\n",
    "        self.class_to_idx = {class_folder: i for i, class_folder in enumerate(self.class_folders)}\n",
    "        self.video_files, self.labels, self.subjects = self._find_video_files_and_labels()\n",
    "        self.class_names = self.class_folders\n",
    "\n",
    "        # Use FullDatasetCache for the entire dataset\n",
    "        self._cached_videos = FullDatasetCache()\n",
    "\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        # Create a mapping from subjects to classes\n",
    "        self.subject_to_class = self._create_subject_to_class_mapping()\n",
    "\n",
    "        # Unpack transform and brightness if provided\n",
    "        if self.transform is not None:\n",
    "            self.transform, self.consistent_brightness = self.transform\n",
    "        else:\n",
    "            self.transform, self.consistent_brightness = None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.reset_brightness()  # Reset brightness for each video\n",
    "        if self._cached_videos[idx][0] is None:\n",
    "            video_file = self.video_files[idx]\n",
    "            frames, label = self.load_video(video_file)\n",
    "            if frames is None or label is None:\n",
    "                return None, None\n",
    "\n",
    "            frames_pil = [Image.fromarray(frame) for frame in frames]\n",
    "\n",
    "             # Apply consistent brightness to all frames\n",
    "            frames_brightened = self.consistent_brightness(frames_pil)\n",
    "\n",
    "             # Apply other transformations if available\n",
    "            if self.transform:\n",
    "                frames_transformed = [self.transform(frame) for frame in frames_brightened]\n",
    "            else:\n",
    "                frames_transformed = frames_brightened\n",
    "\n",
    "\n",
    "            video_tensor = torch.stack([frame for frame in frames_transformed])\n",
    "\n",
    "            if label not in self.class_to_idx:\n",
    "                print(f\"Warning: Label '{label}' not found in class folders.\")\n",
    "                return None, None\n",
    "\n",
    "            label_tensor = torch.tensor(self.class_to_idx[label], dtype=torch.long).to(self.device)\n",
    "\n",
    "            self._cached_videos[idx] = (video_tensor, label_tensor)\n",
    "\n",
    "        return self._cached_videos[idx]\n",
    "\n",
    "    def reset_brightness(self):\n",
    "        self.consistent_brightness.reset()\n",
    "\n",
    "    def _find_video_files_and_labels(self):\n",
    "        video_files = []\n",
    "        labels = []\n",
    "        subjects = []\n",
    "        for class_folder in self.class_folders:\n",
    "            class_folder_path = os.path.join(self.dataset_folder, class_folder)\n",
    "            if not os.path.isdir(class_folder_path):\n",
    "                continue\n",
    "            for root, _, files in os.walk(class_folder_path):\n",
    "                for file in files:\n",
    "                    if os.path.splitext(file)[1].lower() == '.avi':  # Ensure correct extension\n",
    "                        video_files.append(os.path.join(root, file))\n",
    "                        labels.append(class_folder)\n",
    "                        subject = self.extract_subject_from_file(file)\n",
    "                        subjects.append(subject)\n",
    "        return video_files, labels, subjects\n",
    "\n",
    "    def extract_subject_from_file(self, filename):\n",
    "        \"\"\"\n",
    "        Extracts the subject name from the video filename.\n",
    "        Assumes the subject name is the part after the last underscore.\n",
    "        \"\"\"\n",
    "        subject = filename.split('_')[-1]\n",
    "        return subject\n",
    "\n",
    "    def load_video(self, video_file):\n",
    "        frames = []\n",
    "        label = None\n",
    "\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_file)\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "\n",
    "            if len(frames) == 0:\n",
    "                print(f\"No frames found in video {video_file}\")\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the video {video_file}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            label = os.path.basename(os.path.dirname(video_file))\n",
    "            if label not in self.class_folders:\n",
    "                print(f\"Label '{label}' not recognized for video {video_file}\")\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while getting the label for the video {video_file}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        return frames, label\n",
    "    def _create_subject_to_class_mapping(self):\n",
    "        \"\"\"\n",
    "        Create a mapping from subjects to their respective class indices.\n",
    "        \"\"\"\n",
    "        subject_to_class = {}\n",
    "        for idx, subject in enumerate(self.subjects):\n",
    "            label = self.labels[idx]\n",
    "            subject_to_class[subject] = self.class_to_idx[label]\n",
    "        return subject_to_class\n",
    "\n",
    "    def get_subject_class(self, subject):\n",
    "        \"\"\"\n",
    "        Get the class index for a given subject.\n",
    "        \"\"\"\n",
    "        return self.subject_to_class.get(subject, None)\n",
    "\n",
    "def collate_wrapper_with_mask(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "\n",
    "    if any(video is None for video in videos):\n",
    "        videos = [video for video in videos if video is not None]\n",
    "        labels = [label for label in labels if label is not None]\n",
    "\n",
    "    if not videos:\n",
    "        return torch.empty(0, 0, 0, 0), torch.empty(0, 0), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    max_frames = max(video.shape[0] for video in videos)\n",
    "\n",
    "    padded_videos = []\n",
    "    masks = []\n",
    "    for video in videos:\n",
    "        pad_size = max_frames - video.shape[0]\n",
    "        if pad_size > 0:\n",
    "            padding = torch.zeros((pad_size, *video.shape[1:]), dtype=video.dtype, device=video.device)\n",
    "            padded_video = torch.cat((video, padding), dim=0)\n",
    "            mask = torch.cat((torch.ones(video.shape[0], dtype=torch.float32, device=video.device),\n",
    "                              torch.zeros(pad_size, dtype=torch.float32, device=video.device)), dim=0)\n",
    "        else:\n",
    "            padded_video = video\n",
    "            mask = torch.ones(video.shape[0], dtype=torch.float32, device=video.device)\n",
    "        padded_videos.append(padded_video)\n",
    "        masks.append(mask)\n",
    "\n",
    "    videos_tensor = torch.stack(padded_videos)\n",
    "    masks_tensor = torch.stack(masks)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return videos_tensor, masks_tensor, labels_tensor\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean, dtype=torch.float32).view(1, 3, 1, 1)  # Ensure the shape matches (1, 3, 1, 1)\n",
    "        self.std = torch.tensor(std, dtype=torch.float32).view(1, 3, 1, 1)    # Ensure the shape matches (1, 3, 1, 1)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return (tensor.float() - self.mean.to(tensor.device)) / self.std.to(tensor.device)\n",
    "\n",
    "\n",
    "class InitializeMediaPipe:\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.8)\n",
    "\n",
    "mp_initializer = InitializeMediaPipe()\n",
    "\n",
    "class R2Plus1DFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True, dropout_prob=0.0):\n",
    "        super(R2Plus1DFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.dropout_prob = dropout_prob if dropout_prob is not None else 0.0\n",
    "        # Initialize the model with pretrained weights if specified\n",
    "        self.initialize_model()\n",
    "\n",
    "        # Add Spatial Attention Module\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        if self.pretrained:\n",
    "            self.pretrained_model = r2plus1d_18(weights=R2Plus1D_18_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.pretrained_model = r2plus1d_18(weights=None)  # Load model without pretrained weights\n",
    "\n",
    "        # Extract layers from the model\n",
    "        self.stem = self.pretrained_model.stem\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=(0, 0, 0))\n",
    "        self.layer1 = nn.Sequential(*self.pretrained_model.layer1)\n",
    "        self.layer2 = nn.Sequential(*self.pretrained_model.layer2)\n",
    "        self.layer3 = nn.Sequential(*self.pretrained_model.layer3)\n",
    "        self.layer4 = nn.Sequential(*self.pretrained_model.layer4)\n",
    "        self.pretrained_model.fc = nn.Identity()  # Remove the FC layer\n",
    "\n",
    "         # Conditionally add dropout layers\n",
    "        self.dropout = nn.Dropout3d(self.dropout_prob) if self.dropout_prob > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, spatial_masks=None, padding_mask=None, use_spatial_attention=None):\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        # Apply the initial layers\n",
    "        x = self.stem(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Apply the spatial mask if specified\n",
    "        if spatial_masks is not None:\n",
    "            output_size = (x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "            spatial_masks = F.interpolate(spatial_masks.unsqueeze(1), size=output_size, mode='trilinear', align_corners=True)\n",
    "            spatial_masks = spatial_masks.expand(-1, x.size(1), -1, -1, -1)\n",
    "            x = x * spatial_masks\n",
    "\n",
    "        # Apply padding mask if provided\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1).float()\n",
    "            x = x * padding_mask\n",
    "\n",
    "        # Apply subsequent layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer1\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer2\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer3\n",
    "\n",
    "        if use_spatial_attention is True:\n",
    "            # Apply Spatial Attention after layer3\n",
    "            x = self.spatial_attention(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer4\n",
    "\n",
    "        # Save the features after applying the spatial mask\n",
    "        return x\n",
    "\n",
    "    def reload_weights(self):\n",
    "        # Reinitialize the model weights\n",
    "        self.initialize_model()\n",
    "\n",
    "class R3DFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True, dropout_prob=0.0):\n",
    "        super(R3DFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.dropout_prob = dropout_prob if dropout_prob is not None else 0.0\n",
    "\n",
    "        # Initialize the model with pretrained weights if specified\n",
    "        self.initialize_model()\n",
    "\n",
    "        # Add Spatial Attention Module\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        if self.pretrained:\n",
    "            self.pretrained_model = r3d_18(weights=R3D_18_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.pretrained_model = r3d_18(weights=None)  # Load model without pretrained weights\n",
    "\n",
    "        # Extract layers from the model\n",
    "        self.stem = self.pretrained_model.stem\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=(0, 0, 0))\n",
    "        self.layer1 = nn.Sequential(*self.pretrained_model.layer1)\n",
    "        self.layer2 = nn.Sequential(*self.pretrained_model.layer2)\n",
    "        self.layer3 = nn.Sequential(*self.pretrained_model.layer3)\n",
    "        self.layer4 = nn.Sequential(*self.pretrained_model.layer4)\n",
    "        self.pretrained_model.fc = nn.Identity()  # Remove the FC layer\n",
    "\n",
    "        # Conditionally add dropout layers\n",
    "        self.dropout = nn.Dropout3d(self.dropout_prob) if self.dropout_prob > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, spatial_masks=None, padding_mask=None, use_spatial_attention=None):\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        # Apply the initial layers\n",
    "        x = self.stem(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Apply the spatial mask if specified\n",
    "        if spatial_masks is not None:\n",
    "            output_size = (x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "            spatial_masks = F.interpolate(spatial_masks.unsqueeze(1), size=output_size, mode='trilinear', align_corners=True)\n",
    "            spatial_masks = spatial_masks.expand(-1, x.size(1), -1, -1, -1)\n",
    "            x = x * spatial_masks\n",
    "\n",
    "        # Apply padding mask if provided\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1).float()\n",
    "            x = x * padding_mask\n",
    "\n",
    "        # Apply subsequent layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer1\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer2\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer3\n",
    "\n",
    "        if use_spatial_attention is True:\n",
    "            # Apply Spatial Attention after layer3\n",
    "            x = self.spatial_attention(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.dropout(x)  # Apply dropout after layer4\n",
    "\n",
    "        # Save the features after applying the spatial mask\n",
    "        return x\n",
    "\n",
    "    def reload_weights(self):\n",
    "        # Reinitialize the model weights\n",
    "        self.initialize_model()\n",
    "\n",
    "# Define the Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is 5D (batch_size, channels, depth, height, width)\n",
    "        if len(x.shape) != 5:\n",
    "            raise ValueError(f\"Expected input to be 5D (batch_size, channels, depth, height, width), but got shape {x.shape}\")\n",
    "\n",
    "        # Squeeze-and-Excitation block\n",
    "        batch_size, channels, depth, height, width = x.shape\n",
    "\n",
    "        avg_pool = F.adaptive_avg_pool3d(x, 1).view(batch_size, channels)\n",
    "        max_pool = F.adaptive_max_pool3d(x, 1).view(batch_size, channels)\n",
    "\n",
    "        avg_out = self.fc2(F.relu(self.fc1(avg_pool)))\n",
    "        max_out = self.fc2(F.relu(self.fc1(max_pool)))\n",
    "        out = avg_out + max_out\n",
    "        attention_weights = self.sigmoid(out).view(batch_size, channels, 1, 1, 1)\n",
    "        attended_features = x * attention_weights\n",
    "\n",
    "        return attended_features, attention_weights\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        # Convolutional layer to generate the attention map\n",
    "        self.conv1 = nn.Conv3d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute average and max-pooling along the channel dimension\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)  # Average pooling across channels\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # Max pooling across channels\n",
    "\n",
    "        # Concatenate avg_out and max_out along the channel dimension\n",
    "        out = torch.cat([avg_out, max_out], dim=1)  # Shape: (batch_size, 2, depth, height, width)\n",
    "\n",
    "        # Apply convolution and sigmoid activation to generate the attention map\n",
    "        out = self.conv1(out)\n",
    "        attention_map = self.sigmoid(out)  # Shape: (batch_size, 1, depth, height, width)\n",
    "\n",
    "        # Apply attention map to the original feature map\n",
    "        return x * attention_map  # Shape: (batch_size, channels, depth, height, width)\n",
    "\n",
    "\n",
    "class MicroExpressionClassifier(nn.Module):\n",
    "    def __init__(self, d_model=512, num_classes=3, dropout_prob=0.0, pretrained=True, use_spatial_attention=None, use_spatial_masks=None, use_channel_attention=False, feature_extractor_cls=None):\n",
    "        super(MicroExpressionClassifier, self).__init__()\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "        self.feature_extractor = feature_extractor_cls(pretrained=pretrained, dropout_prob=dropout_prob)\n",
    "        self.use_channel_attention = use_channel_attention\n",
    "        self.use_spatial_masks = use_spatial_masks\n",
    "        self.use_spatial_attention = use_spatial_attention\n",
    "\n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        if self.use_channel_attention:\n",
    "            self.channel_attention = ChannelAttention(in_channels=512)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        if self.use_spatial_masks:\n",
    "            # Extract facial regions and generate spatial masks\n",
    "            facial_regions = self._extract_facial_regions(x)\n",
    "            spatial_masks = self._generate_spatial_masks(x, facial_regions)\n",
    "            features = self.feature_extractor(x, spatial_masks, padding_mask, self.use_spatial_attention)\n",
    "        else:\n",
    "            features = self.feature_extractor(x, None, padding_mask)\n",
    "\n",
    "        # Global Average Pooling across spatial dimensions\n",
    "        features = F.adaptive_avg_pool3d(features, (1, 1, 1))\n",
    "\n",
    "        # Apply Channel Attention if enabled\n",
    "        if self.use_channel_attention:\n",
    "            features, attn_weights = self.channel_attention(features)\n",
    "\n",
    "        # Flatten before classification\n",
    "        features = features.view(features.size(0), -1)\n",
    "        outputs = self.classifier(features)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def _extract_facial_regions(self, x, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Extracts head outline points for each frame in the batch using Mediapipe's face mesh.\n",
    "        \"\"\"\n",
    "        facial_regions_list = []\n",
    "        batch_size, num_frames, channels, height, width = x.size()\n",
    "\n",
    "        # Downscale the frames for faster processing\n",
    "        downscale_factor = 0.5  # Adjust the factor as needed\n",
    "        down_height, down_width = int(height * downscale_factor), int(width * downscale_factor)\n",
    "        downscaled_frames = F.interpolate(x.view(-1, channels, height, width), size=(down_height, down_width), mode='bilinear')\n",
    "\n",
    "        head_outline_indices = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378,\n",
    "                                400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21,\n",
    "                                54, 103, 67, 109]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            facial_regions_batch = []\n",
    "            for j in range(num_frames):\n",
    "                if padding_mask is not None and padding_mask[i, j] == 0:\n",
    "                    # Skip padded frames\n",
    "                    facial_regions_batch.append([])\n",
    "                    continue\n",
    "\n",
    "                frame = downscaled_frames[i * num_frames + j].permute(1, 2, 0).cpu().numpy()  # Minimal detach\n",
    "                frame = (frame * 255).astype(np.uint8)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                results_mesh = self.face_mesh.process(rgb_frame)\n",
    "\n",
    "                if results_mesh.multi_face_landmarks:\n",
    "                    landmarks = results_mesh.multi_face_landmarks[0]\n",
    "                    points = []\n",
    "\n",
    "                    # Extract head outline points\n",
    "                    for idx in head_outline_indices:\n",
    "                        x_coord = int(landmarks.landmark[idx].x * down_width)\n",
    "                        y_coord = int(landmarks.landmark[idx].y * down_height)\n",
    "                        points.append((x_coord, y_coord))\n",
    "\n",
    "                    if points:\n",
    "                        # Scale back the points to the original resolution\n",
    "                        points = [(int(x / downscale_factor), int(y / downscale_factor)) for x, y in points]\n",
    "                        facial_regions_batch.append(points)\n",
    "                    else:\n",
    "                        facial_regions_batch.append([])\n",
    "                else:\n",
    "                    facial_regions_batch.append([])\n",
    "\n",
    "            facial_regions_list.append(facial_regions_batch)\n",
    "\n",
    "        return facial_regions_list\n",
    "\n",
    "    def _generate_spatial_masks(self, x, facial_regions, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Generates a spatial mask for each frame based on the detected head outline,\n",
    "        and fills missing frames with the average mask.\n",
    "        \"\"\"\n",
    "        batch_size, num_frames, channels, height, width = x.size()\n",
    "        combined_spatial_mask = torch.ones((batch_size, num_frames, height, width), device=x.device)  # Initialize with ones\n",
    "\n",
    "        shrink_factor = 0.93  # Adjust how much the mask moves inward\n",
    "\n",
    "        # To store masks for calculating the average\n",
    "        valid_masks = []\n",
    "\n",
    "        for i, tensor in enumerate(facial_regions):\n",
    "            batch_spatial_mask = torch.zeros((num_frames, height, width), device=x.device)  # Initialize with zeros\n",
    "            for j, regions in enumerate(tensor):\n",
    "                if padding_mask is not None and padding_mask[i, j] == 0:\n",
    "                    # Skip padded frames\n",
    "                    continue\n",
    "\n",
    "                if regions:\n",
    "                    # Process each frame and generate the mask\n",
    "                    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "                    points = np.array(regions, np.int32)\n",
    "\n",
    "                    # Calculate the center of the head outline\n",
    "                    center_x = np.mean(points[:, 0])\n",
    "                    center_y = np.mean(points[:, 1])\n",
    "\n",
    "                    # Shrink the points towards the center based on the shrink_factor\n",
    "                    for k in range(len(points)):\n",
    "                        points[k][0] = int(center_x + shrink_factor * (points[k][0] - center_x))\n",
    "                        points[k][1] = int(center_y + shrink_factor * (points[k][1] - center_y))\n",
    "\n",
    "                    # Create the head outline mask using fillPoly\n",
    "                    cv2.fillPoly(mask, [points], 255)  # Fill the region inside the head outline\n",
    "\n",
    "                    # Convert mask to torch tensor and normalize to [0, 1]\n",
    "                    individual_mask = torch.tensor(mask, dtype=torch.float32, device=x.device) / 255.0\n",
    "                    batch_spatial_mask[j] = individual_mask\n",
    "\n",
    "                    # Add valid mask to list\n",
    "                    valid_masks.append(individual_mask)\n",
    "                else:\n",
    "                    batch_spatial_mask[j] = torch.zeros((height, width), device=x.device)  # Placeholder for missing mask\n",
    "\n",
    "            combined_spatial_mask[i] = batch_spatial_mask\n",
    "\n",
    "        # Compute average mask across valid masks\n",
    "        if valid_masks:\n",
    "            avg_mask = torch.stack(valid_masks).mean(dim=0)\n",
    "        else:\n",
    "            avg_mask = torch.ones((height, width), device=x.device)  # Fallback in case no valid masks at all\n",
    "\n",
    "        # Apply average mask to frames that were skipped\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_frames):\n",
    "                if combined_spatial_mask[i, j].sum() == 0:  # Check if mask is zero (empty)\n",
    "                    combined_spatial_mask[i, j] = avg_mask  # Assign average mask\n",
    "\n",
    "        return combined_spatial_mask\n",
    "\n",
    "\n",
    "    def plot_channel_attention(self, attention_weights):\n",
    "        # Convert to NumPy array and ensure it's 1-dimensional\n",
    "        attention_weights_np = attention_weights.detach().cpu().numpy()\n",
    "        if attention_weights_np.ndim > 1:\n",
    "            attention_weights_np = attention_weights_np.flatten()\n",
    "\n",
    "        # Plot attention weights\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(attention_weights_np.size), attention_weights_np, color='blue')\n",
    "        plt.xlabel('Channel')\n",
    "        plt.ylabel('Attention Weight')\n",
    "        plt.title('Channel Attention Weights')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_attention_weights(self):\n",
    "        # Extract attention weights from the last transformer layer\n",
    "        if self.transformer_module:\n",
    "            last_transformer_layer = self.transformer_module.transformer[-1]\n",
    "            return last_transformer_layer.attn_weights\n",
    "        else:\n",
    "            raise RuntimeError(\"No Transformer module is present.\")\n",
    "\n",
    "\n",
    "    def visualize_feature_maps(self, feature_maps):\n",
    "        # Convert feature maps to numpy and plot\n",
    "        feature_maps = feature_maps.cpu().detach().numpy()\n",
    "        batch_size, channels, depth, height, width = feature_maps.shape\n",
    "\n",
    "        # Plot feature maps for the first sample in the batch\n",
    "        for i in range(channels):\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            for d in range(depth):\n",
    "                plt.subplot(1, depth, d + 1)\n",
    "                plt.imshow(feature_maps[0, i, d, :, :], cmap='viridis')\n",
    "                plt.axis('off')\n",
    "            plt.suptitle(f'Channel {i}')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def visualize_attention_weights(self, attention_weights, sequence_range=0):\n",
    "        print(f\"Attention Weights shape: {attention_weights.shape}\")\n",
    "        # Check the shape of the attention_weights\n",
    "        for sequence_index in range(sequence_range):\n",
    "            if len(attention_weights.shape) == 3:\n",
    "                # If the shape is [num_sequences, sequence_length, sequence_length]\n",
    "\n",
    "                attention_matrix = attention_weights[sequence_index, :, :]\n",
    "\n",
    "                print(f\"Attention Matrix shape: {attention_matrix.shape}\")\n",
    "\n",
    "                plt.imshow(attention_matrix, cmap='viridis')\n",
    "                plt.title(f'Attention Weights for Batch {sequence_index}')\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported attention_weights shape\")\n",
    "\n",
    "def draw_facial_regions(frame, regions):\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    colors = {\n",
    "            \"eyes\": (255, 0, 0),  # Blue in BGR\n",
    "            # \"nose\": (0, 255, 0),  # Green\n",
    "            \"chin\": (0, 0, 255),  # Red\n",
    "            # \"forehead\": (255, 255, 0),  # Cyan\n",
    "            \"mouth_nose\": (255, 0, 255)  # Magenta\n",
    "        }\n",
    "\n",
    "    print(\"regions\", regions)\n",
    "    for _ in regions:\n",
    "\n",
    "        print(\"DRAWing FUNC\", _)\n",
    "        for x_min, y_min, x_max, y_max in _:\n",
    "\n",
    "            # print(\"DRAW FUNC\", x_min, y_min, x_max, y_max)\n",
    "            # Convert relative coordinates to absolute pixels\n",
    "            h, w, _ = frame_copy.shape\n",
    "            x_min, y_min = int(x_min * w), int(y_min * h)\n",
    "            x_max, y_max = int(x_max * w), int(y_max * h)\n",
    "\n",
    "\n",
    "            print(\"shape\", h, w)\n",
    "            print(\"act\", x_min, y_min, x_max, y_max)\n",
    "\n",
    "            cv2.rectangle(frame_copy, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "    return frame_copy\n",
    "\n",
    "def save_metrics_to_file(metrics, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def check_grad_flow(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                print(f'{name}: {param.grad.abs().mean().item()}')\n",
    "            else:\n",
    "                print(f'{name}: No gradient')\n",
    "\n",
    "def is_valid_excel_file(file_path):\n",
    "    try:\n",
    "        pd.read_excel(file_path, engine='openpyxl')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def get_existing_configurations(existing_results):\n",
    "    return set(zip(existing_results['test_name'], existing_results['fold']))\n",
    "\n",
    "def load_existing_results(excel_file):\n",
    "    if os.path.exists(excel_file):\n",
    "        return pd.read_excel(excel_file)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\n",
    "            'test_name', 'fold', 'epoch', 'avg_train_loss', 'train_accuracy',\n",
    "            'train_precision', 'train_recall', 'train_f1_score', 'train_balanced_accuracy',\n",
    "            'train_pr_auc', 'avg_val_loss', 'val_accuracy', 'val_precision', 'val_recall',\n",
    "            'val_f1_score', 'val_balanced_accuracy', 'val_pr_auc'\n",
    "        ])\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "# Early stopping mechanism\n",
    "def early_stopping(epochs_without_improvement, patience):\n",
    "    return epochs_without_improvement > patience\n",
    "\n",
    "def print_metrics(epoch, train_metrics, val_metrics, avg_train_loss, avg_val_loss):\n",
    "    \"\"\"\n",
    "    Print the training and validation metrics for the current epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - epoch: Current epoch number.\n",
    "    - train_metrics: Dictionary of training metrics.\n",
    "    - val_metrics: Dictionary of validation metrics.\n",
    "    - avg_train_loss: Average training loss for the current epoch.\n",
    "    - avg_val_loss: Average validation loss for the current epoch.\n",
    "    \"\"\"\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Training metrics\n",
    "    print(\"Training Metrics:\")\n",
    "    print(f\"  Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {train_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {train_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {train_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  PR AUC: {train_metrics.get('pr_auc', 0.0):.4f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Validation metrics\n",
    "    print(\"Validation Metrics:\")\n",
    "    print(f\"  Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {val_metrics['f1']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  PR AUC: {val_metrics.get('pr_auc', 0.0):.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "\n",
    "def save_to_excel(ablation_results, results_dir):\n",
    "    columns = [\n",
    "        'test_name', 'fold', 'epoch', 'avg_train_loss', 'train_accuracy',\n",
    "        'train_precision', 'train_recall', 'train_f1_score', 'train_balanced_accuracy',\n",
    "        'train_pr_auc', 'avg_val_loss', 'val_accuracy', 'val_precision', 'val_recall',\n",
    "        'val_f1_score', 'val_balanced_accuracy', 'val_pr_auc'\n",
    "    ]\n",
    "    rows = []\n",
    "\n",
    "    for test_name, results in ablation_results.items():\n",
    "        for fold, fold_results in enumerate(results, start=1):\n",
    "            for result in fold_results:\n",
    "                row = [\n",
    "                    test_name,\n",
    "                    fold,\n",
    "                    result['epoch'],\n",
    "                    result['avg_train_loss'],\n",
    "                    result['train_accuracy'],\n",
    "                    result['train_precision'],\n",
    "                    result['train_recall'],\n",
    "                    result['train_f1_score'],\n",
    "                    result['train_balanced_accuracy'],\n",
    "                    result['train_pr_auc'],\n",
    "                    result['avg_val_loss'],\n",
    "                    result['val_accuracy'],\n",
    "                    result['val_precision'],\n",
    "                    result['val_recall'],\n",
    "                    result['val_f1_score'],\n",
    "                    result['val_balanced_accuracy'],\n",
    "                    result['val_pr_auc']\n",
    "                ]\n",
    "                rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    excel_filename = os.path.join(results_dir, 'hyper_ablation_results.xlsx')\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(excel_filename):\n",
    "            existing_df = pd.read_excel(excel_filename)\n",
    "            df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "        print(f\"Results saved to {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the results: {e}\")\n",
    "\n",
    "def print_final_results(ablation_results):\n",
    "    for test_name, results in ablation_results.items():\n",
    "        if not results:\n",
    "            print(f\"No results for test: {test_name}\")\n",
    "            continue\n",
    "\n",
    "        avg_training_loss = np.mean([res.get('training_loss', np.nan) for res in results])\n",
    "        avg_accuracy = np.mean([res.get('accuracy', np.nan) for res in results])\n",
    "        avg_precision = np.mean([res.get('precision', np.nan) for res in results])\n",
    "        avg_recall = np.mean([res.get('recall', np.nan) for res in results])\n",
    "        avg_f1 = np.mean([res.get('f1_score', np.nan) for res in results])\n",
    "        avg_balanced_accuracy = np.mean([res.get('balanced_accuracy', np.nan) for res in results])\n",
    "        avg_train_pr_auc = np.mean([res.get('train_pr_auc', np.nan) for res in results])\n",
    "        avg_val_pr_auc = np.mean([res.get('val_pr_auc', np.nan) for res in results])\n",
    "\n",
    "        print(f\"Test: {test_name}\")\n",
    "        print(f\"Average Training Loss: {avg_training_loss:.2f}\")\n",
    "        print(f\"Average Accuracy: {avg_accuracy:.2f}\")\n",
    "        print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "        print(f\"Average Recall: {avg_recall:.2f}\")\n",
    "        print(f\"Average F1-score: {avg_f1:.2f}\")\n",
    "        print(f\"Average Balanced Accuracy: {avg_balanced_accuracy:.2f}\")\n",
    "        print(f\"Average Training PR AUC: {avg_train_pr_auc:.2f}\")\n",
    "        print(f\"Average Validation PR AUC: {avg_val_pr_auc:.2f}\")\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names, results_dir, test_name, fold, mode='train'):\n",
    "    \"\"\"\n",
    "    Plots and saves the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        true_labels (list): True labels of the data.\n",
    "        predictions (list): Predicted labels of the data.\n",
    "        class_names (list): List of class names.\n",
    "        results_dir (str): Directory to save the plot.\n",
    "        test_name (str): Name of the test configuration.\n",
    "        fold (int): Current fold number.\n",
    "        mode (str): 'train' for training confusion matrix, 'val' for validation.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=range(len(class_names)))\n",
    "    cm_normalized = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]  # Normalize confusion matrix\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix - {test_name} - Fold {fold + 1} - {mode.capitalize()}')\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plot_filename = f'{test_name}_fold{fold + 1}_{mode}_confusion_matrix.png'\n",
    "    plot_filepath = os.path.join(results_dir, plot_filename)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filepath)\n",
    "    plt.close()\n",
    "\n",
    "    print(f'{mode.capitalize()} confusion matrix saved to: {plot_filepath}')\n",
    "\n",
    "def ablation_configurations():\n",
    "    ablation_tests = {\n",
    "        # # # Updated R2Plus1D Configurations\n",
    "        # # # Dropout 0.3 and Spatial Attention, No Channel Attention, Spatial Masks Enabled\n",
    "        'R2Plus1D_Dropout0.3_SpatialAttention_SpatialMasks': {\n",
    "            'model': MicroExpressionClassifier(\n",
    "                d_model=512,\n",
    "                num_classes=3,\n",
    "                dropout_prob=0.3,  # Updated dropout rate\n",
    "                use_spatial_attention=True,\n",
    "                pretrained=True,\n",
    "                use_spatial_masks=True,\n",
    "                use_channel_attention=False,\n",
    "                feature_extractor_cls=R2Plus1DFeatureExtractor\n",
    "            ),\n",
    "            'description': 'R2Plus1D feature extractor with dropout rate 0.3, Spatial Attention enabled, Channel Attention disabled, and spatial masks enabled.'\n",
    "        },\n",
    "\n",
    "        # # # Dropout 0.2 and Spatial Attention, No Channel Attention, Spatial Masks Enabled\n",
    "        'R2Plus1D_Dropout0.2_SpatialAttention_SpatialMasks': {\n",
    "            'model': MicroExpressionClassifier(\n",
    "                d_model=512,\n",
    "                num_classes=3,\n",
    "                dropout_prob=0.2,  # Updated dropout rate\n",
    "                use_spatial_attention=True,\n",
    "                pretrained=True,\n",
    "                use_spatial_masks=True,\n",
    "                use_channel_attention=False,\n",
    "                feature_extractor_cls=R2Plus1DFeatureExtractor\n",
    "            ),\n",
    "            'description': 'R2Plus1D feature extractor with dropout rate 0.2, Spatial Attention enabled, Channel Attention disabled, and spatial masks enabled.'\n",
    "        },\n",
    "\n",
    "        # # # Channel Attention Configurations (Dropout 0.2, No Spatial Attention, Channel Attention Enabled, Spatial Masks Enabled)\n",
    "        'R2Plus1D_Dropout0.2_NoSpatialAttention_ChanAttn_SpatialMasks': {\n",
    "            'model': MicroExpressionClassifier(\n",
    "                d_model=512,\n",
    "                num_classes=3,\n",
    "                dropout_prob=0.2,  # Updated dropout rate\n",
    "                use_spatial_attention=False,\n",
    "                pretrained=True,\n",
    "                use_spatial_masks=True,\n",
    "                use_channel_attention=True,\n",
    "                feature_extractor_cls=R2Plus1DFeatureExtractor\n",
    "            ),\n",
    "            'description': 'R2Plus1D feature extractor with dropout rate 0.2, Spatial Attention disabled, Channel Attention enabled, and spatial masks enabled.'\n",
    "        },\n",
    "\n",
    "    }\n",
    "    return ablation_tests\n",
    "\n",
    "def log_metrics(writer, phase, loss, metrics, epoch):\n",
    "    writer.add_scalar(f'{phase}/Loss', loss, epoch)\n",
    "    for metric, value in metrics.items():\n",
    "        writer.add_scalar(f'{phase}/{metric.capitalize()}', value, epoch)\n",
    "\n",
    "def log_per_class_metrics(writer, epoch, y_true, y_pred, phase):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    for i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "        writer.add_scalar(f'{phase}/Class_{i}_Precision', p, epoch)\n",
    "        writer.add_scalar(f'{phase}/Class_{i}_Recall', r, epoch)\n",
    "        writer.add_scalar(f'{phase}/Class_{i}_F1_Score', f, epoch)\n",
    "    print(f\"{phase} - Epoch {epoch} per-class metrics:\")\n",
    "    for i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "        print(f\"Class {i}: Precision: {p}, Recall: {r}, F1-Score: {f}\")\n",
    "\n",
    "def log_confusion_matrix(writer, epoch, y_true, y_pred, class_names, phase):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(cm, cmap='Blues')\n",
    "    plt.title(f'{phase} Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set x and y ticks with labels\n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_yticks(np.arange(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(class_names)\n",
    "\n",
    "    # Set the labels for the axes\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Adjust layout to ensure labels fit\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Iterate over data dimensions and create text annotations\n",
    "    threshold = cax.get_array().max() / 2  # Use the maximum value to set the threshold for text color\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        text_color = 'white' if cm[i, j] > threshold else 'black'\n",
    "        ax.text(j, i, val, ha='center', va='center', color=text_color)\n",
    "\n",
    "    # Log the confusion matrix as a figure to TensorBoard\n",
    "    writer.add_figure(f'{phase}/Confusion_Matrix', fig, epoch)\n",
    "\n",
    "    # Close the figure to avoid memory issues\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Print the confusion matrix for reference\n",
    "    print(f\"{phase} - Epoch {epoch} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "def log_roc_pr_curves(writer, epoch, y_true, y_probs, class_names, phase):\n",
    "    y_true = np.array(y_true)\n",
    "    y_probs = np.array(y_probs)\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Ensure y_true is an array-like and y_probs is a 2D array\n",
    "        if y_true.ndim == 1 and y_probs.ndim == 2 and y_probs.shape[1] == len(class_names):\n",
    "            # ROC Curve\n",
    "            fpr, tpr, _ = roc_curve(y_true == i, y_probs[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (class {class_name}) (area = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve for {class_name}')\n",
    "            plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2))  # Move legend to the bottom\n",
    "            plt.tight_layout()  # Adjust layout to make everything fit\n",
    "            writer.add_figure(f'{phase}/ROC_Curve_{class_name}', plt.gcf(), epoch)\n",
    "            plt.close()\n",
    "\n",
    "            # Precision-Recall Curve\n",
    "            precision, recall, _ = precision_recall_curve(y_true == i, y_probs[:, i])\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (class {class_name}) (area = {pr_auc:.2f})')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title(f'Precision-Recall Curve for {class_name}')\n",
    "            plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.2))  # Move legend to the bottom\n",
    "            plt.tight_layout()  # Adjust layout to make everything fit\n",
    "            writer.add_figure(f'{phase}/PR_Curve_{class_name}', plt.gcf(), epoch)\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"Invalid input dimensions for class {class_name}: y_true.ndim = {y_true.ndim}, y_probs.ndim = {y_probs.ndim}\")\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_probs=None):\n",
    "    \"\"\"\n",
    "    Compute metrics for multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Array-like of true labels.\n",
    "    - y_pred: Array-like of predicted labels.\n",
    "    - y_probs: Array-like of predicted probabilities (for ROC AUC and PR AUC).\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary of computed metrics.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    metrics = {}\n",
    "\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['f1'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if y_probs is not None:\n",
    "        y_probs = np.array(y_probs)\n",
    "        num_classes = len(np.unique(y_true))\n",
    "\n",
    "        # Ensure y_probs is a 2D array\n",
    "        if y_probs.ndim == 1:\n",
    "            raise ValueError(\"y_probs should be a 2D array with shape (num_samples, num_classes)\")\n",
    "\n",
    "        # Adjust the shape of y_probs if necessary\n",
    "        if y_probs.shape[1] != num_classes:\n",
    "            if y_probs.shape[1] > num_classes:\n",
    "                y_probs = y_probs[:, :num_classes]  # Truncate extra columns\n",
    "            elif y_probs.shape[1] < num_classes:\n",
    "                padding = np.zeros((y_probs.shape[0], num_classes - y_probs.shape[1]))\n",
    "                y_probs = np.hstack((y_probs, padding))\n",
    "\n",
    "        # Normalize y_probs to ensure they sum to 1 across classes for each sample\n",
    "        y_probs = y_probs / y_probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Compute ROC AUC\n",
    "        metrics['roc_auc'] = roc_auc_score(y_true, y_probs, multi_class='ovr', average='weighted')\n",
    "\n",
    "        # Compute Precision-Recall AUC\n",
    "        pr_auc = 0\n",
    "        for i in range(num_classes):\n",
    "            true_binary = (y_true == i).astype(int)\n",
    "            pred_binary = y_probs[:, i]\n",
    "            precision, recall, _ = precision_recall_curve(true_binary, pred_binary)\n",
    "            pr_auc += auc(recall, precision)\n",
    "        pr_auc /= num_classes\n",
    "        metrics['pr_auc'] = pr_auc\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train_one_fold(train_loader, val_loader, model, criterion, optimizer, scheduler, num_epochs, device, writer, fold, patience, results_dir, dataset, checkpoint_dir, test_name,accumulation_steps):\n",
    "    best_metric = -float('inf')  # Initialize with a very low value for F1 score\n",
    "    epochs_without_improvement = 0\n",
    "    fold_result = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        y_true_train = []\n",
    "        y_pred_train = []\n",
    "        y_probs_train = []\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Training Phase\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\") as pbar:\n",
    "            for i, (inputs, padding_mask, labels) in enumerate(train_loader):\n",
    "                inputs, padding_mask, labels = inputs.to(device, non_blocking=True), padding_mask.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(inputs, padding_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = loss / accumulation_steps\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                running_train_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_true_train.extend(labels.cpu().numpy())\n",
    "                y_pred_train.extend(preds.cpu().numpy())\n",
    "                y_probs_train.extend(F.softmax(outputs, dim=1).detach().cpu().numpy())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'Loss': loss.item()})\n",
    "                pbar.refresh()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_metrics = compute_metrics(y_true_train, y_pred_train, np.array(y_probs_train))\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        y_true_val = []\n",
    "        y_pred_val = []\n",
    "        y_probs_val = []\n",
    "\n",
    "        # Validation Phase\n",
    "        with torch.no_grad(), tqdm(total=len(val_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\") as pbar:\n",
    "            for inputs, padding_mask, labels in val_loader:\n",
    "                inputs, padding_mask, labels = inputs.to(device, non_blocking=True), padding_mask.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                outputs = model(inputs, padding_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_true_val.extend(labels.cpu().numpy())\n",
    "                y_pred_val.extend(preds.cpu().numpy())\n",
    "                y_probs_val.extend(F.softmax(outputs, dim=1).detach().cpu().numpy())\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'Loss': loss.item()})\n",
    "                pbar.refresh()\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_metrics = compute_metrics(y_true_val, y_pred_val, np.array(y_probs_val))\n",
    "        avg_val_f1 = val_metrics['f1']  # Extract the F1 score for early stopping\n",
    "\n",
    "        # Log Metrics\n",
    "        log_metrics(writer, 'Training', avg_train_loss, train_metrics, epoch)\n",
    "        log_metrics(writer, 'Validation', avg_val_loss, val_metrics, epoch)\n",
    "        log_per_class_metrics(writer, epoch, y_true_train, y_pred_train, phase='Train')\n",
    "        log_per_class_metrics(writer, epoch, y_true_val, y_pred_val, phase='Validation')\n",
    "        log_confusion_matrix(writer, epoch, y_true_train, y_pred_train, class_names=[class_name for class_name, _ in sorted(dataset.class_to_idx.items(), key=lambda x: x[1])], phase='Train')\n",
    "        log_confusion_matrix(writer, epoch, y_true_val, y_pred_val, class_names=[class_name for class_name, _ in sorted(dataset.class_to_idx.items(), key=lambda x: x[1])], phase='Validation')\n",
    "        log_roc_pr_curves(writer, epoch, y_true_train, np.array(y_probs_train), class_names=[class_name for class_name, _ in sorted(dataset.class_to_idx.items(), key=lambda x: x[1])], phase='Train')\n",
    "        log_roc_pr_curves(writer, epoch, y_true_val, np.array(y_probs_val), class_names=[class_name for class_name, _ in sorted(dataset.class_to_idx.items(), key=lambda x: x[1])], phase='Validation')\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Metrics:\")\n",
    "        print(f\"  Training - Accuracy: {train_metrics['accuracy']:.4f}, Precision: {train_metrics['precision']:.4f}, Recall: {train_metrics['recall']:.4f}, F1 Score: {train_metrics['f1']:.4f}, Balanced Accuracy: {train_metrics['balanced_accuracy']:.4f}, PR AUC: {train_metrics['pr_auc']:.4f}\")\n",
    "        print(f\"  Validation - Accuracy: {val_metrics['accuracy']:.4f}, Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1 Score: {val_metrics['f1']:.4f}, Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}, PR AUC: {val_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "        metrics = {\n",
    "            'epoch': epoch,\n",
    "            'avg_train_loss': avg_train_loss,\n",
    "            'train_accuracy': train_metrics['accuracy'],\n",
    "            'train_precision': train_metrics['precision'],\n",
    "            'train_recall': train_metrics['recall'],\n",
    "            'train_f1_score': train_metrics['f1'],\n",
    "            'train_balanced_accuracy': train_metrics['balanced_accuracy'],\n",
    "            'train_pr_auc': train_metrics['pr_auc'],\n",
    "            'avg_val_loss': avg_val_loss,\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1_score': val_metrics['f1'],\n",
    "            'val_balanced_accuracy': val_metrics['balanced_accuracy'],\n",
    "            'val_pr_auc': val_metrics['pr_auc']\n",
    "        }\n",
    "\n",
    "        fold_result.append(metrics)\n",
    "\n",
    "        # Step the scheduler based on the current epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_f1 > best_metric:  # Compare using F1 score\n",
    "            best_metric = avg_val_f1\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'{test_name}_fold_{fold}.pth'))\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement > patience:\n",
    "            print(\"Early stopping due to no improvement\")\n",
    "            break\n",
    "\n",
    "    return fold_result\n",
    "\n",
    "def reinitialize_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # Linear layers are initialized using Kaiming uniform by default\n",
    "        nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n",
    "        if module.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(module.bias, -bound, bound)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, ChannelAttention):\n",
    "        nn.init.kaiming_uniform_(module.fc1.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(module.fc2.weight, a=math.sqrt(5))\n",
    "    elif isinstance(module, SpatialAttention):\n",
    "        # Initialize SpatialAttention\n",
    "        nn.init.kaiming_uniform_(module.conv1.weight, a=math.sqrt(5))\n",
    "    # Add more initializations as needed for other layer types\n",
    "\n",
    "# Function to group subjects by class and count their samples\n",
    "def group_subjects_by_class(dataset):\n",
    "    class_subject_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for idx, label in enumerate(dataset.labels):\n",
    "        subject = dataset.subjects[idx]\n",
    "        class_subject_counts[label][subject] += 1\n",
    "    return class_subject_counts\n",
    "\n",
    "def allocate_subjects_unique(class_subject_counts, train_ratio=0.8, max_spno_val=2):\n",
    "    train_subjects, val_subjects = set(), set()\n",
    "    train_class_counts, val_class_counts = defaultdict(int), defaultdict(int)\n",
    "    total_samples_per_class = {label: sum(subject_counts.values()) for label, subject_counts in class_subject_counts.items()}\n",
    "    train_samples_target_per_class = {label: int(total * train_ratio) for label, total in total_samples_per_class.items()}\n",
    "\n",
    "    spno_subjects_per_class = defaultdict(int)  # Track 'spNO' subjects per class for validation\n",
    "    spno_subjects_to_train = defaultdict(set)  # Track 'spNO' subjects to be considered for training\n",
    "\n",
    "    def add_subjects_to_set(subject_set, other_set, target_class_counts, source_class_counts, is_validation=False):\n",
    "        nonlocal spno_subjects_per_class, spno_subjects_to_train  # Use the outer scope variables\n",
    "\n",
    "        for label in source_class_counts.keys():\n",
    "            sorted_subjects = sorted(source_class_counts[label].items(), key=lambda x: x[1], reverse=True)\n",
    "            for subject, count in sorted_subjects:\n",
    "                if subject in other_set:\n",
    "                    continue\n",
    "\n",
    "                # For validation set, limit 'spNO' subjects per class\n",
    "                if is_validation and \"spNO\" in subject:\n",
    "                    if spno_subjects_per_class[label] >= max_spno_val:\n",
    "                        # Track 'spNO' subjects to be added to training set\n",
    "                        spno_subjects_to_train[label].add(subject)\n",
    "                        continue\n",
    "                    else:\n",
    "                        spno_subjects_per_class[label] += 1  # Increment the 'spNO' subject count for this class in validation\n",
    "\n",
    "                # Normal allocation logic for training/validation\n",
    "                if subject_set == train_subjects:\n",
    "                    if target_class_counts[label] + count <= train_samples_target_per_class[label]:\n",
    "                        subject_set.add(subject)\n",
    "                        target_class_counts[label] += count\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if target_class_counts[label] + count <= total_samples_per_class[label] - train_samples_target_per_class[label]:\n",
    "                        subject_set.add(subject)\n",
    "                        target_class_counts[label] += count\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "    # Allocate subjects to training set first\n",
    "    print(\"Allocating subjects to training set...\")\n",
    "    add_subjects_to_set(train_subjects, val_subjects, train_class_counts, class_subject_counts)\n",
    "    print(f\"Training subjects: {train_subjects}\")\n",
    "    print(f\"Training class counts: {dict(train_class_counts)}\")\n",
    "\n",
    "    # Allocate subjects to validation set\n",
    "    print(\"Allocating subjects to validation set...\")\n",
    "    add_subjects_to_set(val_subjects, train_subjects, val_class_counts, class_subject_counts, is_validation=True)\n",
    "    print(f\"Validation subjects: {val_subjects}\")\n",
    "    print(f\"Validation class counts: {dict(val_class_counts)}\")\n",
    "\n",
    "    # Ensure 'spNO' subjects in validation not used are added to training\n",
    "    for label, subjects in spno_subjects_to_train.items():\n",
    "        for subject in subjects:\n",
    "            if subject not in train_subjects and subject not in val_subjects:\n",
    "                train_subjects.add(subject)\n",
    "                # Update train class counts\n",
    "                subject_count = class_subject_counts[label][subject]\n",
    "                train_class_counts[label] += subject_count\n",
    "\n",
    "    # Ensure that the subjects in each set are unique\n",
    "    assert not (train_subjects & val_subjects), \"Subjects overlap between training and validation sets\"\n",
    "\n",
    "    return train_subjects, val_subjects, train_class_counts, val_class_counts\n",
    "\n",
    "# Function to sample a specific number of samples from each class\n",
    "def sample_from_each_class(indices, labels, num_samples_per_class):\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx in indices:\n",
    "        label = labels[idx]\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    sampled_indices = []\n",
    "    for label, indices_list in class_indices.items():\n",
    "        if len(indices_list) < num_samples_per_class:\n",
    "            raise ValueError(f\"Not enough samples in class {label} to sample {num_samples_per_class} samples.\")\n",
    "\n",
    "        sampled_indices.extend(random.sample(indices_list, num_samples_per_class))\n",
    "\n",
    "    return sampled_indices\n",
    "\n",
    "# Function to print subjects for a given set of indices\n",
    "def print_subjects_for_indices(indices, dataset):\n",
    "    subjects = [dataset.subjects[i] for i in indices]\n",
    "    subject_counts = defaultdict(int)\n",
    "    for subject in subjects:\n",
    "        subject_counts[subject] += 1\n",
    "\n",
    "    print(\"Subjects and their counts:\")\n",
    "    for subject, count in subject_counts.items():\n",
    "        print(f\"  Subject {subject}: {count} videos\")\n",
    "\n",
    "# Function to print subjects and classes for a given set of indices\n",
    "def print_subjects_by_class(indices, dataset):\n",
    "    subjects_by_class = defaultdict(set)\n",
    "    for i in indices:\n",
    "        label = dataset.labels[i]\n",
    "        subject = dataset.subjects[i]\n",
    "        subjects_by_class[label].add(subject)\n",
    "\n",
    "    for label, subjects in subjects_by_class.items():\n",
    "        print(f\"Class {label}:\")\n",
    "        for subject in sorted(subjects):\n",
    "            print(f\"  Subject {subject}\")\n",
    "\n",
    "\n",
    "# Ensure consistent seeding for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_class_weights_and_sampler(labels):\n",
    "    \"\"\"Compute class weights and sampler for the imbalanced dataset.\"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_numeric = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Compute class sample counts\n",
    "    class_sample_counts = np.bincount(labels_numeric)\n",
    "    total_samples = len(labels_numeric)\n",
    "    num_classes = len(class_sample_counts)\n",
    "\n",
    "    # Compute class weights\n",
    "    # Adjust class weights to be inversely proportional to class frequencies\n",
    "    class_weights = total_samples / (num_classes * class_sample_counts)\n",
    "\n",
    "    # Normalize class weights if needed\n",
    "    class_weights = class_weights / np.max(class_weights)  # Normalization to [0, 1]\n",
    "\n",
    "    # Assign weights to each sample\n",
    "    sample_weights = class_weights[labels_numeric]\n",
    "\n",
    "    # Create a weighted sampler\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # Convert class weights to tensor for use in loss functions\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    return class_weights_tensor, sampler, label_encoder\n",
    "\n",
    "\n",
    "# Print the video count per subject for each class\n",
    "def print_videos_per_subject_per_class(dataset):\n",
    "    class_subjects = defaultdict(lambda: defaultdict(int))\n",
    "    for idx, (video_file, label) in enumerate(zip(dataset.video_files, dataset.labels)):\n",
    "        subject = dataset.extract_subject_from_file(os.path.basename(video_file))\n",
    "        class_index = dataset.class_to_idx.get(label, None)\n",
    "        if class_index is not None:\n",
    "            class_subjects[class_index][subject] += 1\n",
    "\n",
    "    # Print subjects and video counts per class\n",
    "    print(\"Subjects per class and their video counts:\")\n",
    "    for cls in sorted(class_subjects.keys()):\n",
    "        print(f\"Class {cls}:\")\n",
    "        for subject, count in class_subjects[cls].items():\n",
    "            print(f\"  Subject {subject}: {count} videos\")\n",
    "        total_videos = sum(class_subjects[cls].values())\n",
    "        print(f\"  Total videos for Class {cls}: {total_videos}\")\n",
    "\n",
    "def print_class_distribution(set_name, indices, labels):\n",
    "    class_labels = [labels[i] for i in indices]\n",
    "    distribution = compute_class_distribution(class_labels)\n",
    "    print(f\"{set_name} class distribution: {distribution}\")\n",
    "\n",
    "def compute_class_distribution(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Example of calculating class distributions for a fold\n",
    "def print_fold_distributions(train_indices, val_indices, dataset):\n",
    "    train_labels = [dataset.labels[idx] for idx in train_indices]\n",
    "    val_labels = [dataset.labels[idx] for idx in val_indices]\n",
    "\n",
    "    train_distribution = compute_class_distribution(train_labels)\n",
    "    val_distribution = compute_class_distribution(val_labels)\n",
    "\n",
    "    print(f\"Training set class distribution: {train_distribution}\")\n",
    "    print(f\"Validation set class distribution: {val_distribution}\")\n",
    "\n",
    "class ConsistentRandomBrightness:\n",
    "    def __init__(self, min_brightness=0.95, max_brightness=1.05):\n",
    "        self.min_brightness = min_brightness\n",
    "        self.max_brightness = max_brightness\n",
    "        self.brightness_factor = None  # This will store the random brightness factor for a given sequence\n",
    "\n",
    "    def __call__(self, video_frames):\n",
    "        # If the brightness factor has not been set for the current sequence, generate one\n",
    "        if self.brightness_factor is None:\n",
    "            self.brightness_factor = random.uniform(self.min_brightness, self.max_brightness)\n",
    "        # Apply the same brightness factor to all frames\n",
    "        return [TR.adjust_brightness(frame, self.brightness_factor) for frame in video_frames]\n",
    "\n",
    "    def reset(self):\n",
    "        # Call this to reset the brightness factor between sequences\n",
    "        self.brightness_factor = None\n",
    "\n",
    "def seeded_transform(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    consistent_brightness = ConsistentRandomBrightness(0.95, 1.05)\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(contrast=0.1, saturation=0.1, hue=0.2),\n",
    "        transforms.ToTensor(),\n",
    "    ]), consistent_brightness\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Class weights (1D tensor)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Cross-entropy loss (per-sample loss)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        # print(f\"Cross-Entropy Loss (ce_loss): {ce_loss}\")\n",
    "\n",
    "        # Probabilities from cross-entropy\n",
    "        probs = torch.exp(-ce_loss)  # Equivalent to probabilities of the correct class\n",
    "        # print(f\"Probabilities for target class (pt): {probs}\")\n",
    "\n",
    "        # Focal weight: (1 - pt)^gamma\n",
    "        focal_weight = (1 - probs) ** self.gamma\n",
    "        # print(f\"Focal Weight: {focal_weight}\")\n",
    "\n",
    "        # Apply class weights (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            # Gather alpha values for each target class\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            # print(f\"Alpha (class weights) for each target: {alpha_t}\")\n",
    "            focal_loss = alpha_t * focal_weight * ce_loss\n",
    "        else:\n",
    "            focal_loss = focal_weight * ce_loss\n",
    "\n",
    "        # print(f\"Focal Loss before reduction: {focal_loss}\")\n",
    "\n",
    "        # Return the mean focal loss across the batch\n",
    "        final_loss = focal_loss.mean()\n",
    "        # print(f\"Final Focal Loss (mean): {final_loss}\")\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "def get_optimizer_and_scheduler(model, train_loader, pretrained_lr, non_pretrained_lr, min_lr, weight_decay, total_epochs):\n",
    "    def extract_parameters(extractor):\n",
    "        return {\n",
    "            'pretrained': [p for n, p in extractor.pretrained_model.named_parameters()],\n",
    "            'spatial_attention': [p for n, p in extractor.spatial_attention.named_parameters()]\n",
    "        }\n",
    "\n",
    "    params = {}\n",
    "    if hasattr(model.feature_extractor, 'pretrained_model') and hasattr(model.feature_extractor, 'spatial_attention'):\n",
    "        params.update(extract_parameters(model.feature_extractor))\n",
    "\n",
    "    if model.use_channel_attention and hasattr(model, 'channel_attention'):\n",
    "        params['channel_attention'] = [p for n, p in model.channel_attention.named_parameters()]\n",
    "\n",
    "    if hasattr(model, 'classifier'):\n",
    "        params['classifier'] = [p for n, p in model.classifier.named_parameters()]\n",
    "\n",
    "    # Define parameter groups with their specific learning rates (global weight decay applies to all groups)\n",
    "    param_groups = [\n",
    "        {'params': params.get('pretrained', []), 'lr': pretrained_lr},  # Pretrained layers\n",
    "        {'params': params.get('spatial_attention', []), 'lr': non_pretrained_lr}, # Non-pretrained layers\n",
    "        {'params': params.get('channel_attention', []), 'lr': non_pretrained_lr},\n",
    "        {'params': params.get('classifier', []), 'lr': non_pretrained_lr}\n",
    "    ]\n",
    "\n",
    "    # Check for duplicate parameters\n",
    "    all_params = sum(params.values(), [])\n",
    "    if len(set(all_params)) != len(all_params):\n",
    "        raise ValueError(\"Some parameters appear in more than one parameter group\")\n",
    "\n",
    "    # Create optimizer with global weight decay applied uniformly to all parameter groups\n",
    "    optimizer = torch.optim.AdamW(param_groups, weight_decay=weight_decay)\n",
    "\n",
    "    # Automatically calculate total steps based on DataLoader\n",
    "    num_batches_per_epoch = len(train_loader)  # Number of batches per epoch\n",
    "    total_steps = num_batches_per_epoch * total_epochs  # Total steps for all epochs\n",
    "\n",
    "    # Adjust the max learning rates to reduce instability\n",
    "    max_lrs = [\n",
    "        pretrained_lr * 1.5,    # Conservative for pretrained layers\n",
    "        non_pretrained_lr * 1.5, # More aggressive for non-pretrained layers\n",
    "        non_pretrained_lr * 1.5,\n",
    "        non_pretrained_lr * 1.5\n",
    "    ]\n",
    "    # OneCycleLR scheduler with separate max learning rates for each parameter group\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "          optimizer,\n",
    "          max_lr=max_lrs,\n",
    "          total_steps=total_steps,\n",
    "          pct_start=0.3,            # Start decay earlier\n",
    "          anneal_strategy='cos',  # Switch to linear annealing for smoother decay\n",
    "          final_div_factor=20        # Reduce learning rate more at the end\n",
    "      )\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def train_model():\n",
    "    seed = 42\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    NUM_CLASSES = 3\n",
    "    BATCH_SIZE = 4\n",
    "    VAL_BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 15\n",
    "    PRETRAINED_LR = 2e-4        # Increase learning rate\n",
    "    BASE_LR = 1e-3             # Increase learning rate\n",
    "    MIN_LR = 1e-6\n",
    "    PATIENCE = NUM_EPOCHS\n",
    "    GAMMA_LOSS = 1.0         # Lower gamma for focal loss, or try CrossEntropyLoss\n",
    "    ACCUMULATION_STEPS = 2      # Reduce gradient accumulation to update more frequently\n",
    "    WEIGHT_DECAY = 1e-4  # Weight decay\n",
    "\n",
    "    # File paths\n",
    "    path = \"/content/drive/MyDrive/COMPOSITE_CASMEII\"\n",
    "    dataset_name = \"COMPOSITE_3_CLASS_DATASET\"\n",
    "    checkpoint_dir = os.path.join(path, \"ablation\", \"checkpoints\")\n",
    "    results_dir = os.path.join(path, \"ablation\", \"results\")\n",
    "    log_dir = os.path.join(results_dir, \"tensorboard_logs\")\n",
    "    results_file = os.path.join(results_dir, 'ablation_results.xlsx')\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    existing_configurations = get_existing_configurations(existing_results)\n",
    "\n",
    "    # Helper function to create directories\n",
    "    def create_directories():\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    create_directories()\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_folder = os.path.join(path, dataset_name)\n",
    "    transform = seeded_transform(seed)\n",
    "    dataset = VideoDataset(dataset_folder, transform=transform, seed=seed)\n",
    "    print(f\"Total videos in dataset: {len(dataset)}\")\n",
    "\n",
    "    # Print parameters\n",
    "    print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "    print(f\"Batch size (train): {BATCH_SIZE}\")\n",
    "    print(f\"Batch size (validation): {VAL_BATCH_SIZE}\")\n",
    "    print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"Base Learning rate: {BASE_LR}\")\n",
    "    print(f\"Pretrained Learning rate: {PRETRAINED_LR}\")\n",
    "    print(f\"Minimum learning rate: {MIN_LR}\")\n",
    "    print(f\"Early stopping patience (epochs): {PATIENCE}\")\n",
    "    print(f\"Gamma value for focal loss: {GAMMA_LOSS}\")\n",
    "    print(f\"Gradient accumulation steps: {ACCUMULATION_STEPS}\")\n",
    "\n",
    "    # Prepare Stratified K-Fold\n",
    "    labels = dataset.labels\n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    def print_fold_distributions():\n",
    "        class_subject_counts = group_subjects_by_class(dataset)\n",
    "        for fold_num, (train_indices, val_indices) in enumerate(skf.split(np.arange(len(labels)), labels), 1):\n",
    "            train_subjects, val_subjects, _, _ = allocate_subjects_unique(class_subject_counts, train_ratio=0.85)\n",
    "            filtered_train_indices = [i for i in train_indices if dataset.subjects[i] in train_subjects]\n",
    "            filtered_val_indices = [i for i in val_indices if dataset.subjects[i] in val_subjects]\n",
    "            print(f\"\\nFold {fold_num} Distribution:\")\n",
    "            print_class_distribution(\"Training set\", filtered_train_indices, labels)\n",
    "            print_class_distribution(\"Validation set\", filtered_val_indices, labels)\n",
    "\n",
    "    print_fold_distributions()\n",
    "\n",
    "    ablation_tests = ablation_configurations()\n",
    "    ablation_results = {name: [] for name in ablation_tests.keys()}\n",
    "\n",
    "    for config_name, config in ablation_tests.items():\n",
    "        print(f\"\\nUsing configuration: {config_name}\")\n",
    "\n",
    "        for fold_num, (train_indices, val_indices) in enumerate(skf.split(np.arange(len(labels)), labels), 1):\n",
    "            print(f\"\\nProcessing Fold {fold_num} with configuration {config_name}...\")\n",
    "\n",
    "            # Check if the configuration and fold already exist\n",
    "            if (config_name, fold_num) in existing_configurations:\n",
    "                print(f\"Skipping Fold {fold_num} with configuration {config_name} as it already exists.\")\n",
    "                continue\n",
    "\n",
    "            class_subject_counts = group_subjects_by_class(dataset)\n",
    "            train_subjects, val_subjects, _, _ = allocate_subjects_unique(class_subject_counts, train_ratio=0.85)\n",
    "            filtered_train_indices = [i for i in train_indices if dataset.subjects[i] in train_subjects]\n",
    "            filtered_val_indices = [i for i in val_indices if dataset.subjects[i] in val_subjects]\n",
    "\n",
    "            # Print subjects and classes\n",
    "            print_subjects_by_class(filtered_train_indices, dataset)\n",
    "            print_subjects_by_class(filtered_val_indices, dataset)\n",
    "            print_class_distribution(\"Training set\", filtered_train_indices, labels)\n",
    "            print_class_distribution(\"Validation set\", filtered_val_indices, labels)\n",
    "\n",
    "            train_labels = [dataset.labels[idx] for idx in filtered_train_indices]\n",
    "            class_weights, train_sampler, _ = compute_class_weights_and_sampler(train_labels)\n",
    "            print(\"Class weights:\", class_weights)\n",
    "\n",
    "            val_labels = [dataset.labels[idx] for idx in filtered_val_indices]\n",
    "            val_class_distribution = compute_class_distribution(val_labels)\n",
    "            print(f\"Validation set class distribution: {val_class_distribution}\")\n",
    "\n",
    "            # DataLoaders\n",
    "            train_dataset = Subset(dataset, filtered_train_indices)\n",
    "            val_dataset = Subset(dataset, filtered_val_indices)\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                sampler=train_sampler,\n",
    "                num_workers=1,\n",
    "                pin_memory=True,\n",
    "                collate_fn=collate_wrapper_with_mask\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=VAL_BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=1,\n",
    "                pin_memory=True,\n",
    "                collate_fn=collate_wrapper_with_mask\n",
    "            )\n",
    "\n",
    "            # TensorBoard writer\n",
    "            fold_log_dir = os.path.join(log_dir, config_name, f\"fold_{fold_num}\")\n",
    "            os.makedirs(fold_log_dir, exist_ok=True)\n",
    "            writer = SummaryWriter(log_dir=fold_log_dir)\n",
    "\n",
    "            # Initialize and train the model\n",
    "            model = config['model'].to(device)\n",
    "\n",
    "            if hasattr(model.feature_extractor, 'spatial_attention') and model.feature_extractor.spatial_attention:\n",
    "                model.feature_extractor.spatial_attention.apply(reinitialize_weights)\n",
    "            if hasattr(model, 'channel_attention') and model.channel_attention:\n",
    "                model.channel_attention.apply(reinitialize_weights)\n",
    "            if hasattr(model, 'classifier') and model.classifier:\n",
    "                model.classifier.apply(reinitialize_weights)\n",
    "\n",
    "            # Call the function to get the optimizer and scheduler\n",
    "            optimizer, scheduler = get_optimizer_and_scheduler(\n",
    "                model,\n",
    "                train_loader,\n",
    "                PRETRAINED_LR,\n",
    "                BASE_LR,\n",
    "                MIN_LR,\n",
    "                WEIGHT_DECAY,\n",
    "                NUM_EPOCHS\n",
    "            )\n",
    "            class_weights_alpha = torch.tensor([0.7, 1.3, 1.1], dtype=torch.float)\n",
    "            criterion = FocalLoss(alpha=class_weights_alpha.to(device), gamma=GAMMA_LOSS).to(device)\n",
    "\n",
    "            fold_result = train_one_fold(\n",
    "                train_loader, val_loader, model, criterion, optimizer, scheduler,\n",
    "                NUM_EPOCHS, device, writer, fold=fold_num, patience=PATIENCE,\n",
    "                results_dir=results_dir, dataset=dataset, checkpoint_dir=checkpoint_dir,\n",
    "                test_name=config_name, accumulation_steps=ACCUMULATION_STEPS\n",
    "            )\n",
    "\n",
    "            ablation_results[config_name].append(fold_result)\n",
    "            print(f\"Fold {fold_num} with configuration {config_name} completed. Results:\")\n",
    "            print(fold_result)\n",
    "            writer.close()\n",
    "\n",
    "            cleanup(model, optimizer, scheduler, train_loader, val_loader)\n",
    "\n",
    "        print(f\"Completed all folds with configuration {config_name}.\")\n",
    "        save_to_excel(ablation_results, results_dir)\n",
    "\n",
    "    print(\"All folds and configurations have been processed and results saved.\")\n",
    "\n",
    "def cleanup(model, optimizer, scheduler, train_loader, val_loader):\n",
    "    # Move model to CPU to free GPU memory\n",
    "    # model.cpu()\n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    gc.collect()  # Force garbage collection\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Print model parameters to check for shared weights\n",
    "# def print_model_params(model, fold):\n",
    "#     print(f\"Model parameters for fold {fold}:\")\n",
    "#     for param in model.parameters():\n",
    "#         print(param.data)\n",
    "\n",
    "def print_model_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: mean={param.data.mean()}, std={param.data.std()}, min={param.data.min()}, max={param.data.max()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
